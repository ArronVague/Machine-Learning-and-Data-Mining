# å®éªŒä¸‰ï¼šé€»è¾‘å›å½’

## ç¬¬ä¸€éƒ¨åˆ†ï¼šå‡½æ•°ä»‹ç»

æœºå™¨å­¦ä¹ ä½¿ç”¨çš„æ•°æ®é›†å¯èƒ½å‡ºç°ç¼ºå¤±å€¼æˆ–å¼‚å¸¸å€¼ï¼Œæˆ–è€…æ˜¯æ•°æ®ç±»å‹ä¸é€‚åˆç›´æ¥åº”ç”¨äºæ¨¡å‹è®­ç»ƒã€‚å› æ­¤ï¼Œæ•°æ®é¢„å¤„ç†æ˜¯æœºå™¨å­¦ä¹ è¿‡ç¨‹ä¸­ååˆ†é‡è¦çš„ä¸€ä¸ªéƒ¨åˆ†ã€‚

åœ¨ç¬¬ä¸€éƒ¨åˆ†å­¦ä¹ åˆ°äº†å¦‚ä½•ä½¿ç”¨pythonè¿›è¡Œ

- åˆ¤æ–­ä¸€åˆ—æ˜¯å¦æœ‰ç¼ºå¤±å€¼

- å¯¹æ¯ä¸€åˆ—çš„ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼è¿›è¡Œæ›¿æ¢æ“ä½œ

- å°†æ ‡è®°å€¼ä»å­—ç¬¦ä¸²ç¼–ç¨‹å®¹æ˜“æ“ä½œçš„æ•´æ•°ç±»å‹

- å¯è§†åŒ–åˆ†ç±»å†³ç­–è¾¹ç•Œ

- ç”»æ•£ç‚¹å›¾

## ç¬¬äºŒéƒ¨åˆ†ï¼šé€»è¾‘å›å½’

```python
# å¯¼å…¥æœºå™¨å­¦ä¹ å¸¸ç”¨çš„åº“
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
import matplotlib as mpl
```

### é¢˜ç›®

<span style="color:purple">Irisæ•°æ®é›†ï¼ˆé¸¢å°¾èŠ±å‰æ•°æ®é›†ï¼‰æ˜¯å¸¸ç”¨çš„åˆ†ç±»å®éªŒæ•°æ®é›†,æ˜¯ä¸€ç±»å¤šé‡å˜é‡åˆ†æçš„æ•°æ®é›†ã€‚æˆ‘ä»¬å®éªŒé€‰å–æ•°æ®é›†çš„éƒ¨åˆ†å†…å®¹ï¼ŒåŒ…å«è®­ç»ƒé›†80ä¸ªæ•°æ®æ ·æœ¬å’Œæµ‹è¯•20ä¸ªæ ·æœ¬ã€‚æ¯ä¸ªæ•°æ®æœ‰2ä¸ªå±æ€§ï¼šèŠ±è¼é•¿åº¦($x_1$)ï¼ŒèŠ±è¼å®½åº¦($x_2$)ã€‚é€šè¿‡è¿™2ä¸ªå±æ€§é¢„æµ‹é¸¢å°¾èŠ±å‰å±äºï¼ˆSetosaï¼ŒVersicolourï¼‰äºŒä¸ªç§ç±»ä¸­çš„å“ªä¸€ç±»ã€‚</span>

#### 1ï¼‰

```python
#your code here------
# å°†è®­ç»ƒæ•°æ®é›†'flower_train.csv'ä¸æµ‹è¯•æ•°æ®é›†'flower_test.csv'è½½å…¥åˆ°Dataframeå¯¹è±¡ä¸­ã€‚
train_frame = pd.read_csv("flower_train.csv")
test_frame = pd.read_csv("flower_test.csv")

print(train_frame)
# åˆ¤æ–­è®­ç»ƒé›†ä¸­æ¯åˆ—æ•°æ®æ˜¯å¦æœ‰ç¼ºå¤±å€¼æˆ–è€…ä¸åˆç†çš„æ•°å€¼
print(train_frame.isnull().sum())

# å…ˆå°†0æ›¿æ¢ä¸ºç©ºå€¼
# åˆ©ç”¨pandasä¸­çš„replaceå‡½æ•°å°†æŸä¸€åˆ—çš„æŒ‡å®šå€¼æ›¿æ¢ä¸ºå¦ä¸€ä¸ªå€¼
train_frame[['x1', 'x2']] = train_frame[['x1', 'x2']].replace(0, np.NaN)

x1_column = train_frame['x1']
x2_column = train_frame['x2']

# å°†ç©ºå€¼æ›¿æ¢ä¸ºå¹³å‡å€¼
# æŒ‰ç§ç±»åˆ†ç±»å¹¶è®¡ç®—æ¯ç»„çš„å¹³å‡èŠ±è¼é•¿åº¦å’ŒèŠ±è¼å®½åº¦
mean_x1_by_type = train_frame.groupby('type')['x1'].transform('mean')
mean_x2_by_type = train_frame.groupby('type')['x2'].transform('mean')
# ä½¿ç”¨æ¯ä¸ªç§ç±»ç»„çš„å¹³å‡å€¼æ¥æ›¿æ¢ç¼ºå¤±å€¼
train_frame['x1'].fillna(mean_x1_by_type, inplace=True)
train_frame['x2'].fillna(mean_x2_by_type, inplace=True)

# æŸ¥çœ‹æ˜¯å¦è¿˜æœ‰ç¼ºå¤±å€¼æˆ–ä¸åˆç†çš„æ•°å€¼
print(train_frame)
print(train_frame.isnull().sum())

# å°†æ ‡è®°å€¼ä»å­—ç¬¦ä¸²å˜æˆå®¹æ˜“æ“ä½œçš„æ•´æ•°ç±»å‹
train_frame['type'] = np.where(train_frame['type'] == 'Iris-setosa', 0, 1)
test_frame['type'] = np.where(test_frame['type'] == 'Iris-setosa', 0, 1)
print(train_frame)
print(test_frame)
```

#### 2ï¼‰

```python
# ç›¸åº”åœ°å¾€æµ‹è¯•é›†å’Œè®­ç»ƒé›†æ·»åŠ ğ‘¥0=1è¿™ä¸€ç‰¹å¾
train_frame.insert(0, 'x0', 1)
test_frame.insert(0, 'x0', 1)
print(train_frame)
print(test_frame)
```

#### 3ï¼‰

ç®—æ³•æ­¥éª¤å¦‚ä¸‹ï¼šâ‘ åˆå§‹åŒ–æ¨¡å‹å‚æ•°ğœ”çš„å€¼ï¼›â‘¡åœ¨è´Ÿæ¢¯åº¦çš„æ–¹å‘ä¸Šæ›´æ–°å‚æ•°(ä½¿ç”¨æ‰¹é‡æ¢¯åº¦ä¸‹é™)ï¼Œå¹¶ä¸æ–­è¿­ä»£è¿™ä¸€æ­¥éª¤ã€‚

æ¢¯åº¦çš„ä¸‹é™åå¯¼å…¬å¼ä¸º

$$
\frac{\partial J}{\partial \omega_j}=\frac{1}{m}\sum_{i=1}^m x_{ij}(\frac{e^{\omega^T x_i}}{1+e^{\omega^T x_i}}-y_i)
$$

å‚æ•°æ›´æ–°çš„å…¬å¼ä¸º

$$
\omega_j =\omega_j-\eta\frac{\partial J}{\partial w_j}
$$

å…¶ä¸­$\eta$è¡¨ç¤ºå­¦ä¹ ç‡ï¼Œ$m$åˆ™è¡¨ç¤ºæ‰¹é‡ä¸­çš„æ ·æœ¬æ•°é‡ï¼Œ$x_{ij}$ä»£è¡¨ç€ç¬¬iä¸ªæ ·æœ¬çš„ç¬¬jä¸ªç‰¹å¾å€¼,$y_i$ä»£è¡¨ç€ç¬¬iä¸ªæ ·æœ¬çš„çœŸå®å€¼

```python
# æ‰¹é‡æ¢¯åº¦ä¸‹é™
def batch_gradient_descent(omega_init, sample, learning_rate, threshold):
    def omega_update(omega):
        while True:
            omega_new = omega.copy()
            total_list = [0] * len(sample)
            for i in range(len(sample)):
                wx = omega[0] * sample[i][0] + omega[1] * sample[i][1] + omega[2] * sample[i][2]
                ewx = math.exp(wx)
                total_list[i] = (ewx / (1 + ewx) - sample[i][3])
            for i in range(3):
                s = 0
# æ¢¯åº¦çš„ä¸‹é™åå¯¼å…¬å¼
                for j, total in enumerate(total_list):
                    s += total * sample[j][i]
# å‚æ•°æ›´æ–°çš„å…¬å¼                
                omega_new[i] = omega_new[i] - learning_rate * s / len(sample)
# å½“omega_newå’Œomegaçš„å˜åŒ–ç¨‹åº¦å°äºthresholdæ—¶ç»“æŸè¿­ä»£
            if all(abs(x - y) < threshold for x, y in zip(omega_new, omega)):
                break
            omega = omega_new
        return omega

    omega = omega_update(omega_init)
    return omega

# åˆå§‹åŒ–æ¨¡å‹å‚æ•°omegaçš„å€¼
omega_init = [1, 1, 1]
# å­¦ä¹ ç‡
learning_rate = 0.01
# é˜ˆå€¼
threshold = 0.001

train = np.array(train_frame)
omega = batch_gradient_descent(omega_init, train, learning_rate, threshold)
print(omega)
```

è¾“å‡ºç»“æœ

```powershell
[0.5845696021160349, 0.952909300237262, -1.8537717027300382]
```

#### 4)

æŸå¤±å‡½æ•°

$$
J(\omega)=-\frac{1}{m}\sum^m_{i=1}{lnP(y_i|x_i,\omega)}=-\frac{1}{m}\sum^m_{i=1}ln(y_i\frac{1}{1+e^{-\omega^T x_i}}+(1-y_i)\frac{e^{-\omega^T x_i}}{1+e^{-\omega^T x_i}})
$$

```python
test = np.array(test_frame)
# åœ¨æ¨¡å‹è®­ç»ƒå®Œæˆåå¾—åˆ°æ‰€è®­ç»ƒçš„æ¨¡å‹å‚æ•°omegaï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæ‰€è®­ç»ƒæ¨¡å‹çš„æµ‹è¯•
trained = [omega[0] + omega[1] * x[1] + omega[2] * x[2] for x in test]

# ä½¿ç”¨æŸå¤±å‡½æ•°è®¡ç®—losså€¼
loss = 0
for i in range(len(test)):
    loss += math.log(test[i][3] / (1 + math.exp(-trained[i])) + (1 - test[i][3]) * math.exp(-trained[i]) / (1 + math.exp(-trained[i])))
loss = -loss / len(test)

print(loss)
```

è¾“å‡ºç»“æœ

```powershell
0.3554652826957542
```

#### 5ï¼‰

```python
#ç¡®å®šå›¾ç”»è¾¹ç•Œå’Œå¤§å°
plt.figure(figsize=(10,5))
x_min, x_max = 0,10
y_min, y_max = 0,5
#ä½¿ç”¨numpyä¸­çš„meshgridç”Ÿæˆç½‘æ ¼çŸ©é˜µï¼Œæ–¹ä¾¿è¿›è¡Œä¹‹åçš„æç‚¹
boundary_x, boundary_y = np.meshgrid(np.arange(x_min, x_max, 0.01),np.arange(y_min, y_max, 0.01))
grid = np.c_[boundary_x.ravel(), boundary_y.ravel()]
#åŠ å…¥åç½®(æˆ–w_0)å¯¹åº”çš„ç‰¹å¾ä¸º1çš„ä¸€åˆ—
e=np.ones((len(grid),1))
grid=np.c_[e,grid]
#å‡å®šä¸‹åˆ—çš„æ¨¡å‹å‚æ•°
w=np.array([[omega[0]],[omega[1]],[omega[2]]])
#è®¡ç®—å‡ºç½‘æ ¼ç‚¹ä¸­æ¯ä¸ªç‚¹å¯¹åº”çš„é€»è¾‘å›å½’é¢„æµ‹å€¼
z=grid.dot(w)
for i in range(len(z)):
    z[i][0] = (1 / (1 + np.exp(-z[i][0])))
    if z[i][0]<0.5:
        z[i][0]=0
    else:
        z[i][0]=1
#è½¬æ¢shapeå°†æ‰€å¾—åˆ°çš„é€»è¾‘å›å½’æ¨¡å‹æ‰€å¾—åˆ°çš„å†³ç­–è¾¹ç•Œç»˜åˆ¶å‡ºæ¥
z=z.reshape(boundary_x.shape)
plt.contourf(boundary_x, boundary_y, z, cmap=plt.cm.Spectral, zorder=1)

# æµ‹è¯•é›†çš„æ‰€æœ‰ç‚¹åœ¨åŒä¸€å¹…å›¾ä¸­è¿›è¡Œç»˜åˆ¶
class_1 = test_frame[test_frame['type'] == 1]
class_0 = test_frame[test_frame['type'] == 0]
# ç»™type == 1çš„æµ‹è¯•ç‚¹è“è‰²ï¼Œç»™type == 0çš„æµ‹è¯•ç‚¹çº¢è‰²ï¼Œé€šè¿‡é¢œè‰²çš„åŒºåˆ«ç›´è§‚çœ‹åˆ°é¢„æµ‹æ­£ç¡®å’Œé”™è¯¯çš„æ ·æœ¬
plt.scatter(class_1['x1'],class_1['x2'],c='blue')
plt.scatter(class_0['x1'],class_0['x2'],c='red')
plt.show()
```

è¾“å‡ºç»“æœ

![](pic/result.png)

ä»¥ä¸Šä¸ºå­¦ä¹ ç‡è®¾ç½®ä¸º0.001æ—¶å¾—åˆ°çš„ç»“æœã€‚

å½“å­¦ä¹ ç‡è®¾ç½®ä¸º0.0001æ—¶ï¼Œomegaä¸º

```powershell
[-0.157995008570737, 4.092624517144397, -7.08149697462867]
```

losså€¼ä¸º

```powershell
0.1764902843715746
```

å›¾åƒä¸º

![](pic/result_0.0001.png)

å½“å­¦ä¹ ç‡è®¾ç½®ä¸º0.0001æ—¶ï¼Œomegaä¸º

```powershell
[-3.138366160328139, 10.31193177051994, -16.84028799629563]
```

losså€¼ä¸º

```powershell
0.26850222408959623
```

å›¾åƒä¸º

![](pic/result_0.00001.png)

è§‚å¯Ÿåˆ°ï¼Œå½“thresholdåˆ†åˆ«è®¾ç½®ä¸º0.001ã€0.0001ã€0.00001æ—¶ï¼Œè™½ç„¶losså€¼å…ˆå˜å°å†å˜å¤§ï¼Œä½†æ˜¯å›¾åƒä¸­ï¼Œå†³ç­–è¾¹ç•Œçš„æ–œç‡è¶Šæ¥è¶Šå¤§ã€‚é¢„æµ‹å½“thresholdå°åˆ°ä¸€å®šç¨‹åº¦æ—¶ï¼Œæ–œç‡è¾¾åˆ°ä¸€å®šå€¼æ—¶ï¼Œè“è‰²å’Œçº¢è‰²é¢„æµ‹é”™è¯¯çš„ç‚¹æœ€ç»ˆä¼šè½åˆ°æ­£ç¡®çš„åŒºåŸŸã€‚

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šé€»è¾‘å›å½’å®éªŒäºŒ

### é¢˜ç›®

<span style="color:purple">è¯¥æ•°æ®é›†(train_titanic.csvå’Œtest_titanic.csv)åŒæ ·ä¸ºåˆ†ç±»æ•°æ®é›†ï¼Œä¸ºæ³°å¦å°¼å…‹å·çš„ä¹˜å®¢ä¿¡æ¯ä»¥åŠæœ€åæ˜¯å¦ç”Ÿè¿˜ã€‚æ¯ä¸ªåŒ…æ‹¬äº†ä¸ƒä¸ªç‰¹å¾å€¼ä»¥åŠæ ‡è®°(ä»£è¡¨æ˜¯å¦ç”Ÿè¿˜),ç‰¹å¾ä¿¡æ¯åˆ†åˆ«ä¸ºPassengerid(ä¹˜å®¢id)ï¼ŒAge(ä¹˜å®¢å¹´é¾„)ï¼ŒFare(èˆ¹ç¥¨ä»·æ ¼),Sex(æ€§åˆ«)ï¼Œsibsp(å ‚å…„å¼Ÿå¦¹ä¸ªæ•°)ï¼ŒParch(çˆ¶æ¯ä¸å°å­©çš„ä¸ªæ•°)ï¼ŒPclass(ä¹˜å®¢ç­‰çº§)</span>

<span style="color:purple">è¯¥æ•°æ®é›†å·²ç»åšäº†å¤„ç†ï¼Œæ— ç¼ºå¤±å€¼å’Œç©ºå€¼ï¼Œä¸”å­—ç¬¦ä¸²ç±»å‹å…¨éƒ¨è½¬æ¢æˆäº†æ•´æ•°ç±»å‹ï¼Œä½ ä»¬éœ€è¦è¿›è¡Œåˆ¤æ–­ï¼Œåœ¨ä¸ƒä¸ªç‰¹å¾å€¼è‡³å°‘é€‰æ‹©å››ä¸ªä½ è®¤ä¸ºä¸æœ€åæ˜¯å¦ç”Ÿè¿˜å…³è”åº¦é«˜çš„ç‰¹å¾ç±»åˆ«ã€‚è¯¥å®éªŒçš„ä»»åŠ¡ä¾ç„¶æ˜¯åœ¨è®­ç»ƒé›†ä¸Šä½¿ç”¨é€»è¾‘å›å½’æ–¹æ³•å’Œæ‰‹åŠ¨å®ç°çš„æ¢¯åº¦ä¸‹é™æ–¹æ³•å®Œæˆæ¨¡å‹è®­ç»ƒã€‚</span>

åœ¨æ¥ä¸‹æ¥å®éªŒä¸­ï¼Œæˆ‘é€‰æ‹©äº†å››ä¸ªç‰¹å¾å€¼ï¼Œåˆ†åˆ«æ˜¯Ageï¼ˆä¹˜å®¢å¹´é¾„ï¼‰ï¼ŒSexï¼ˆæ€§åˆ«ï¼‰ï¼ŒParchï¼ˆçˆ¶æ¯ä¸å°å­©çš„ä¸ªæ•°ï¼‰ï¼ŒPclassï¼ˆä¹˜å®¢ï¼‰ã€‚

```python
# å¯¼å…¥æœºå™¨å­¦ä¹ å¸¸ç”¨çš„åº“
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import mathath
```

#### 1ï¼‰

```python
# ä½¿ç”¨pandasåº“å°†è®­ç»ƒé›†'train_titanic.csv'ä¸æµ‹è¯•æ•°æ®é›†'test_titanic.csv'è½½å…¥åˆ°Dataframeå¯¹è±¡ä¸­
train_frame = pd.read_csv("train_titanic.csv")
test_frame = pd.read_csv("test_titanic.csv")
```

#### 2)

åˆ©ç”¨ä¸Šä¸ªå®éªŒæ‰€ä½¿ç”¨çš„æ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼ˆç”±äºä½¿ç”¨éšæœºæ‰¹é‡å’Œå°æ‰¹é‡éƒ½å‘ç”Ÿäº†OverflowErrorï¼Œå› æ­¤ï¼Œæ”¹ç”¨æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼‰

ç®—æ³•æ­¥éª¤å¦‚ä¸‹ï¼šâ‘ åˆå§‹åŒ–æ¨¡å‹å‚æ•°ğœ”çš„å€¼ï¼›â‘¡åœ¨è´Ÿæ¢¯åº¦çš„æ–¹å‘ä¸Šæ›´æ–°å‚æ•°(ä½¿ç”¨æ‰¹é‡æ¢¯åº¦ä¸‹é™)ï¼Œå¹¶ä¸æ–­è¿­ä»£è¿™ä¸€æ­¥éª¤ã€‚

æ¢¯åº¦çš„ä¸‹é™åå¯¼å…¬å¼ä¸º

$$
\frac{\partial J}{\partial \omega_j}=\frac{1}{m}\sum_{i=1}^m x_{ij}(\frac{e^{\omega^T x_i}}{1+e^{\omega^T x_i}}-y_i)
$$

å‚æ•°æ›´æ–°çš„å…¬å¼ä¸º

$$
\omega_j =\omega_j-\eta\frac{\partial J}{\partial w_j}
$$

å…¶ä¸­$\eta$è¡¨ç¤ºå­¦ä¹ ç‡ï¼Œ$m$åˆ™è¡¨ç¤ºæ‰¹é‡ä¸­çš„æ ·æœ¬æ•°é‡ï¼Œ$x_{ij}$ä»£è¡¨ç€ç¬¬iä¸ªæ ·æœ¬çš„ç¬¬jä¸ªç‰¹å¾å€¼,$y_i$ä»£è¡¨ç€ç¬¬iä¸ªæ ·æœ¬çš„çœŸå®å€¼

```python
# ç›¸åº”åœ°å¾€æµ‹è¯•é›†å’Œè®­ç»ƒé›†æ·»åŠ ğ‘¥0=1è¿™ä¸€ç‰¹å¾
train_frame.insert(0, "x0", 1)
test_frame.insert(0, "x0", 1)

# æ‰¹é‡æ¢¯åº¦ä¸‹é™
def batch_gradient_descent(omega_init, sample, learning_rate, threshold):

    def omega_update(omega):
        while True:
            omega_new = omega.copy()
            # print(omega_new)
            total_list = [0] * len(sample)
            # print("hahaha")
            for i in range(len(sample)):
                wx = (
                    omega[0] * sample[i][0]
                    + omega[1] * sample[i][1]
                    + omega[2] * sample[i][2]
                    + omega[3] * sample[i][3]
                    + omega[4] * sample[i][4]
                )
                ewx = math.exp(wx)
                # print(ewx)
                total_list[i] = ewx / (1 + ewx) - sample[i][5]
            # print(total_list)
            for i in range(5):
                s = 0
# æ¢¯åº¦çš„ä¸‹é™åå¯¼å…¬å¼
                for j, total in enumerate(total_list):
                    s += total * sample[j][i]
# å‚æ•°æ›´æ–°çš„å…¬å¼
                omega_new[i] = omega_new[i] - learning_rate * s / len(sample)
            if all(abs(x - y) < threshold for x, y in zip(omega_new, omega)):
                break
            omega = omega_new
        return omega

    omega = omega_update(omega_init)
    return omega

# åˆå§‹åŒ–æ¨¡å‹å‚æ•°omegaçš„å€¼
omega_init = [1, 1, 1, 1, 1]
# å­¦ä¹ ç‡è®¾ç½®ä¸º0.01
learning_rate = 0.01
# é˜ˆå€¼è®¾ç½®ä¸º0.0001
threshold = 0.0001

train = np.array(train_frame)
omega = batch_gradient_descent(omega_init, train, learning_rate, threshold)
print(omega)
```

#### 3ï¼‰

æŸå¤±å‡½æ•°

$$
J(\omega)=-\frac{1}{m}\sum^m_{i=1}{lnP(y_i|x_i,\omega)}=-\frac{1}{m}\sum^m_{i=1}ln(y_i\frac{1}{1+e^{-\omega^T x_i}}+(1-y_i)\frac{e^{-\omega^T x_i}}{1+e^{-\omega^T x_i}})
$$

```python
# ä½¿ç”¨è®­ç»ƒåçš„é€»è¾‘å›å½’æ¨¡å‹å¯¹æµ‹è¯•æ•°æ®é›†'test_titanic.csv'è¿›è¡Œé¢„æµ‹
test = np.array(test_frame)
trained = [
    omega[0] + omega[1] * x[1] + omega[2] * x[2] + omega[3] * x[3] + omega[4] * x[4]
    for x in test
]

# ä½¿ç”¨æŸå¤±å‡½æ•°è®¡ç®—losså€¼
loss = 0
for i in range(len(test)):
    loss += math.log(
        test[i][5] / (1 + math.exp(-trained[i]))
        + (1 - test[i][5]) * math.exp(-trained[i]) / (1 + math.exp(-trained[i]))
    )
loss = -loss / len(test)

print(loss)
```

å–omegaåˆå§‹å€¼ä¸º[3, 3, 3, 3, 3]ï¼Œ

thresholdè®¾ç½®ä¸º0.001æ—¶ï¼Œomegaä¸º

```powershell
[1.85292828630153, -0.07677922855154168, 2.4721323256405565, 0.6185499708649277, -1.0675631218691828]
```

losså€¼ä¸º

```powershell
0.6326159795753409
```

thresholdè®¾ç½®ä¸º0.0001æ—¶ï¼Œomegaä¸º

```powershell
[1.5982342948500974, -0.05515691872753953, 2.115991350403841, -0.09446342211739814, -1.0193405769141177]
```

losså€¼ä¸º

```powershell
0.5556161200534883
```

thresholdè®¾ç½®ä¸º0.00001æ—¶ï¼Œomegaä¸º

```powershell
[0.2621491230570663, -0.03352492331503038, 1.8806578863681949, -0.07688465154355978, -0.6397312133294268]
```

losså€¼ä¸º

```powershell
0.5687583422417055
```

è§‚å¯Ÿåˆ°ï¼Œomega_0çš„å˜åŒ–æœ€å¤§ï¼ˆä¸ºä»€ä¹ˆï¼Ÿï¼‰ï¼Œè€Œä¸”å½“thresholdè®¾ç½®ä¸º0.0001æ—¶ï¼Œlosså€¼æœ€å°ã€‚

æ¥ä¸‹æ¥å›ºå®šthreshold=0.0001ï¼Œç”±äºomegaå˜åŒ–è¾ƒå¤§ï¼Œä¸å†åˆ—å‡ºã€‚

å–omegaåˆå§‹å€¼ä¸º[0, 0, 0, 0, 0]ï¼Œlosså€¼ä¸º

```powershell
0.5741462054082823
```

å–omegaåˆå§‹å€¼ä¸º[1, 1, 1, 1, 1]ï¼Œlosså€¼ä¸º

```powershell
0.565940111995483
```

å–omegaåˆå§‹å€¼ä¸º[2, 2, 2, 2, 2]ï¼Œlosså€¼ä¸º

```powershell
0.5568659743713252
```

å–omegaåˆå§‹å€¼ä¸º[4, 4, 4, 4, 4]ï¼Œlosså€¼ä¸º

```powershell
0.5536965876501643
```

losså€¼é€æ¸å‡å°å¹¶è¶‹äºç¨³å®šã€‚

## æ„Ÿæƒ³

ç¬¬ä¸‰éƒ¨åˆ†é€»è¾‘å›å½’å®éªŒä¸­ï¼ŒAgeè¿™ä¸ªç‰¹å¾å€¼çš„æ•°ç‰¹åˆ«å¤§ï¼Œ æ¢¯åº¦ä¸‹é™æ—¶ï¼Œä½¿ç”¨éšæœºæ‰¹é‡å’Œå°æ‰¹é‡éƒ½å‘ç”Ÿäº†OverflowErrorï¼Œå› æ­¤åªèƒ½æ”¹ç”¨æ‰¹é‡æ¢¯åº¦ä¸‹é™ã€‚

æ„Ÿè§‰è‡ªå·±åœ¨ä»£ç å®ç°çš„æ€è·¯ä¸Šï¼Œè¿˜åœç•™åœ¨å…¶å®ƒè¯­è¨€çš„æ€è·¯ï¼Œæ²¡æœ‰åˆ©ç”¨åˆ°pythonæœ¬èº«çš„ç‰¹æ€§ï¼Œä»¥åŠnumpyåº“é€ å¥½çš„è½®å­ã€‚

ä¸å¤ªæ¸…æ¥šå¦‚ä½•è¯„å®šå®éªŒç»“æœå¥½åï¼Œä¹Ÿä¸å¤ªæ˜ç™½å¦‚ä½•é’ˆå¯¹å®éªŒç»“æœï¼Œè°ƒæ•´å‚æ•°ã€‚åªèƒ½å°½é‡è°ƒæ•´ï¼Œä»è€Œä½¿å¾—losså€¼æœ€å°ã€‚
