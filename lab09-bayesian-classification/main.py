from collections import Counter
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math

# pä»£è¡¨æœ‰æ¯’ï¼Œeä»£è¡¨æ— æ¯’
train_df = pd.read_csv("train_mushroom.csv")
test_df = pd.read_csv("test_mushroom.csv")

train = np.array(train_df)
test = np.array(test_df)

# è®¡ç®—æ¯ä¸ªæ ‡ç­¾å€¼yå¯¹åº”çš„å…ˆéªŒæ¦‚çŽ‡P(y)
# ð‘ƒ(ð‘¦)=|ð·ð‘¦||ð·|

# å…¶ä¸­ ð·ð‘¦
#  ä¸ºæ ‡ç­¾å€¼ä¸ºyçš„æ ·æœ¬é›†åˆï¼Œ |ð·ð‘¦|
#  ä¸ºè¿™ä¸ªé›†åˆçš„æ ·æœ¬ä¸ªæ•°ï¼›Dä¸ºæ‰€æœ‰æ ·æœ¬é›†åˆï¼Œ|D|ä¸ºæ‰€æœ‰æ ·æœ¬ä¸ªæ•°
Dy = len(train)
label_count = Counter(train[:, -1])
priori_probability = {}
for k, v in label_count.items():
    priori_probability[k] = v / Dy
print("before using laplacian smoothing: ", priori_probability)

# 3) å¯¹äºŽæ•°æ®é›†ä¸­çš„æ¯ä¸ªç‰¹å¾çš„éžé‡å¤ç‰¹å¾å€¼ ð‘¥ð‘–
#  ï¼Œè®¡ç®—ç»™å®šæ ‡ç­¾å€¼yæ—¶ç‰¹å¾å€¼ ð‘¥ð‘–
#  çš„æ¡ä»¶æ¦‚çŽ‡ ð‘ƒ(ð‘¥ð‘–â”‚ð‘¦)
#  ,
# ð‘ƒ(ð‘¥ð‘–â”‚ð‘¦)=|ð·ð‘¥ð‘–,ð‘¦||ð·ð‘¦|

# ð·ð‘¥ð‘–,ð‘¦
#  ä¸ºæ ‡ç­¾å€¼ä¸ºyï¼Œç‰¹å¾å€¼ä¸º ð‘¥ð‘–
#  çš„æ ·æœ¬é›†åˆï¼› |ð·ð‘¥ð‘–,ð‘¦|
#  ä¸ºè¯¥é›†åˆçš„æ ·æœ¬ä¸ªæ•°

# é¦–å…ˆéåŽ†æ•°æ®é›†Dä¸­çš„æ¯ä¸ªç‰¹å¾ï¼Œå°†æ¯ä¸ªç‰¹å¾çš„éžé‡å¤å€¼å–å‡º
num_feature = train.shape[1] - 1
feature_unique = [0] * num_feature
for i in range(num_feature):
    feature_unique[i] = set(train[:, i])
# print(feature_unique)

# æ ¹æ®æ ‡ç­¾å€¼å°†æ•°æ®é›†Dåˆ†ä¸ºä¸¤ä¸ªå­æ•°æ®é›†ï¼Œåˆ†åˆ«åŒ…æ‹¬æ‰€æœ‰æ ‡ç­¾å€¼ä¸ºpçš„æ ·æœ¬å’Œæ‰€æœ‰æ ‡ç­¾å€¼ä¸ºeçš„æ ·æœ¬ã€‚
conditional_probability = {}
D = {}
for k in label_count.keys():
    D[k] = train[train[:, -1] == k]


def cal_conditional_probability(D, feature_unique):
    for i in range(num_feature):
        for feature in feature_unique[i]:
            for k in label_count.keys():
                Dxy = D[k][D[k][:, i] == feature]
                conditional_probability[(i, feature, k)] = len(Dxy) / len(D[k])


cal_conditional_probability(D, feature_unique)
# print(conditional_probability)
# print(conditional_probability[(0, "k", "p")])


def pro(a, index):
    res = priori_probability[index]
    for i, x in enumerate(a):
        if (i, x, index) not in conditional_probability.keys():
            return 0
        res *= conditional_probability[(i, x, index)]
    return res


# print(pro(["k", "y", "n", "f", "s", "c", "n", "b", "o", "e", "w", "v", "d"], "e"))
accuracy = 0
for a in test:
    p = pro(a[:-1], "p")
    e = pro(a[:-1], "e")
    predict = "p" if p > e else "e"
    accuracy += 1 if predict == a[-1] else 0

accuracy = accuracy / len(test)
print("before using laplacian smoothing: ", accuracy)

zero = False
for v in conditional_probability.values():
    if v == 0:
        zero = True
        break

# print(zero)
# laplacian_smoothing
priori_probability = {}
for k, v in label_count.items():
    priori_probability[k] = (v + 1) / (Dy + label_count.keys().__len__())
print("after using laplacian smoothing: ", priori_probability)


conditional_probability = {}
D = {}
for k in label_count.keys():
    D[k] = train[train[:, -1] == k]


def cal_conditional_probability(D, feature_unique):
    for i in range(num_feature):
        for feature in feature_unique[i]:
            for k in label_count.keys():
                Dxy = D[k][D[k][:, i] == feature]
                conditional_probability[(i, feature, k)] = (len(Dxy) + 1) / (
                    len(D[k]) + feature_unique[i].__len__()
                )


cal_conditional_probability(D, feature_unique)

accuracy = 0
for a in test:
    p = pro(a[:-1], "p")
    e = pro(a[:-1], "e")
    predict = "p" if p > e else "e"
    accuracy += 1 if predict == a[-1] else 0

accuracy = accuracy / len(test)
print("after using laplacian smoothing: ", accuracy)
